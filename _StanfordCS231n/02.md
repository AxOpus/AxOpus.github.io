---
layout: pagecollection
title: 02 - Image classification
collection: StanfordCS231n
---
{% include JB/setup %}

## Image classification
Receive input data (image), and assign it to one of a (provided) list of categories. The input the computer 'sees' is an $$n \times m \times 3$$ matrix of numbers which is: height in pixels x width in pixels x RGB.

**Semantic gap** is the difference between the idea of a cat and the matrix of pixels the computer sees. Hard problem e.g. imagine we moved the camera on the subject, the entire matrix would change, but it's still the same object.

Problems include: distortions, occlusion, background, etc.

Not a clear, explicit algorithm that can handle these problems. Attempts have been made, for example to compute the edges, corners, etc. Then write down rules based on these features. Not a good solution, not robust.

ML approach is to use a data-driven method:
1. Collect a dataset of images and labels.
2. Use ML to train a classifier.
3. Evaluate the classifier on new images.

## Nearest neighbour
This is a simple classifier which loads the data into memory, then makes predictions based on this data. We can use the L1 distance between the test image and the training image. NN is _slow_ at prediction, but _fast_ at training. This is the opposite of what we want.

We can generalise this to k-nearest neighbours, which takes the k nearest points to classify, based on majority voting.

We can also use different distance metrics such as L2. 

What is the best k? What's the best distance? These are hyperparameters, which we set based on trials.

K-nearest neighbours is never used on images. Slow at test time and distance metrics between pixels is not that useful. Furthermore, there is also the curse of dimensionality, which means we need an exponentially increasing number of images to adequately cover the space.

## Additional materials
- [Image classification](http://cs231n.github.io/classification/)
- [Linear classification](http://cs231n.github.io/linear-classify/)
