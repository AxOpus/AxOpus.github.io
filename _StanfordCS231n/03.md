---
layout: pagecollection
title: 03 - Loss functions and Optimisation
collection: StanfordCS231n
---
{% include JB/setup %}

## Loss functions
We want some measure of how "good" our classifier is. To do this, we use a loss function: $$\mathcal{L}(f(x_i, W), y_i))$$. We now want to minimise this loss function to set our weights.

We look at multiclass SVM loss and softmax. Multiclass SVM loss wants the score for the true class to be higher than the scores for the other classes above some safety margin.

#### Regularisation
We also want to encourage "simple" models. To do this we add an additional function of the weights to the loss function. Examples are L1, L2, max norm, elastic net, dropout, etc. 

## Optimisation
Imagine being in some valleys. We want to find the minimum of these values, which is difficult. We use iterative methods to find the (local) minimum of these valleys.

Specifically we'll use gradient descent. 

## Additional material
- [Optimisation notes](http://cs231n.github.io/optimization-1/)