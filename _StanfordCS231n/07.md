---
layout: pagecollection
title: 07 - Training Convolutional Neural Networks (Part 2)
collection: StanfordCS231n
---
{% include JB/setup %}

## Overview
This lecture continues in the same vein as the previous one. Covering more considerations when using neural networks. We cover:

- Fancier optimisation
- Regularisation
- Transfer learning

## Fancy optimisation
#### Problems with SGD

If the _condiiton number_ is high (shallow gradient in one direction, large gradient in another), then the optimisation will be zig-zaggy.

Also, there are lots of local minima, but even more saddle points, which cause problems with optimisation.

Further, since the gradients come from mini-batches of data, the gradients can be quite noisy.

#### Momentum

We maintain a 'velocity', as a running mean of gradients, and a parameter $\rho$, which acts as friction, usually set to 0.9 or 0.99.

There is also Nesterov momentum, which takes a step in the velocity _then_ estimates the gradient and steps in that direction, compared to evaluating the gradient and velocity, then combining.

#### AdaGrad

We divide by the sum of the squares of the gradient. This means that in a small gradient we are dividing by a small number, which is useful in cases where the difference in gradient directions is large. Not used often.

#### RMSProp

Similar to AdaGrad, we incorporate a decay rate to the squared gradient.

#### Adam

We incorporate momentum, and RMSProp into moments. Further, it adds bias correction, to avoid the large steps that are initially taken.

#### Newton's method

We step directly to the minimum using the _Hessian_ matrix, which is matrix of second-order derivatives. It is rarely used, because we need to store a huge matrix. Sometimes approximations are used, such as L-BFGS.

#### Ensembles
Sometimes it is a good idea to take multiple models and then combine them.

## Regularisation
#### Dropout
We set some neurons to 0 at training time, with probability $p$. A common setting is $p=0.5$.

At test time, we multiply by the probability.

#### Data augmentation
We can randomly transform the data, e.g. flipping, cropping, rotating, varying contrast, lens distortion, shearing, etc.

#### DropConnect
At every forward pass, set some weights to 0 with probability $p$. A common setting is $p=0.5$.

#### Fractional max pooling
Using some randomly sampled elements for the pooling.

#### Stochastic depth
Randomly dropout _layers_. Very recent (madness!).

## Transfer learning
Idea: train on some large dataset. Then 'freeze' the network, and reinitialise the fully-connected layer at output, and retrain that layer. For a bigger dataset, try updating more layers at the end of the network.

Most people will use a base CNN trained on ImageNet. 

Most frameworks will have a 'model zoo' of pre-trained models which are available to download.

## Summary
Initialisation: Use Adam
Regularisation: Use Batch Norm, and Dropout
Transfer learning: Use a pre-trained model


## Additional material
- [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)
