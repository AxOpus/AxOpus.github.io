---
layout: pagecollection
title: 08 - Deep Learning Software
collection: StanfordCS231n
---
{% include JB/setup %}

## CPU vs GPU
NVIDIA cards are used, in general, over AMD for Deep Learning. The overview is that CPU has fewer cores but each core is faster, when compared to GPU (CPU 4-10 cores, GPU 3000+). GPUs also have built-in RAM, and cache.

Matrix multiplication is an ideal candidate for parallelisation, as each element can be computed independently.

Programming GPUs can be done with CUDA, and OpenCL, but is very difficult. In practice you use existing code, rather than writing your own. It is often useful to have cuDNN running rather than vanilla CUDA code.

Data lives on disk, so we may have a bottleneck on reading data. Solutions are to: read all data into RAM; use an SSD instead of an HDD; and use CPU threads to prefetch data.

## Deep learning frameworks
Rapidly changing! In this course we'll focus on three: Caffe2 (and Caffe); PyTorch; and TensorFlow.

The main points of deep learning frameworks are to:
1. Easily compute big computational graphs
2. Easily compute gradients in the graphs
3. Run on GPUs

#### TensorFlow
1. Define the graph
2. Run the graph (again, and again, and...)

See the notebook we are writing.

Keras is a high-level wrapper on top of TensorFlow, which can make building the computational graph simpler. It is one of many, three of which ship with TF itself! An additional package from Google called Pretty Tensor exists, as does DeepMind's version, called Sonnet.

TF is similar to Theano, an earlier framework.

#### PyTorch

Three levels of abstraction:
- Tensor: imperative ndarray, runs on GPU
- Variable: node in a comptational graph
- Module: a neural network layer

TF equivalent:
- Numpy array
- Tensor, Variable, Placeholder
- tf.layers, TFSlim, TFLearn...

Think of PyTorch Tensors as numpy+GPU.

In PyTorch, we build a _dynamic_ computational graph, which means we build a new graph at each iteration of training.

#### Caffe2
New - released this year (2017). Static graphs, Python binding, and runs on iOS and Android.

## Framework competition
Google uses TensorFlow for all parts of its Deep Learning efforts, whereas FaceBook uses PyTorch + Caffe2 for Research and Production, respectively.

## Additional material
- [Intro to parallel programming](https://www.udacity.com/course/intro-to-parallel-programming--cs344)