---
layout: pagecollection
title: 09 - CNN architectures
collection: StanfordCS231n
---
{% include JB/setup %}

## Case studies
- AlexNet
- VGG
- GooLeNet
- ResNet

## AlexNet

Was the first large scale CNN that did well on ImageNet dataset. A series of conv-pool-norm groups, followed by some conv layers, then output.

## VGG
A deeper network than AlexNet (19 vs 8), and with smaller filters (3x3 vs 7x7). Why use smaller filters? Stack of 3x3 conv layers has the same effective receptive field as one 7x7 layer, but deeper with more non-linearities, and with fewer parameters.

In total it has 138 million (!) parameters, compared with 16 million in AlexNet.

## GoogLeNet
Another deeper network (22 layers). Created the 'inception' module, designed to with computational efficiency in mind. 

#### Inception module
Is a 'local network' (a network within a network), that has good local topology, and then stack these on top of each other.

Apply several parallel filter operations and a 3x3 max pooling, and then concatenate these together depth-wise.

Bottleneck layers using 1x1 convolutions are used to reduce depth.

## ResNet
152 layers! Uses the idea of residual connections. If we stack con layers in a regular CNN, the deeper models tend to have _worse_ training error than less deep ones. The hypothesis is that, since this is not overfitting, the problem might be optimisation.

Reasoning: a deeper model should be able to perform at least as well as the shallower model. The residual layers learn a combination of the identity input matrix, and the output of the conv-ReLU-conv stack.

## Other networks to be aware of
- Network-in-network (NiN)
- Identity mappings in deep residual networks
- Wide residual networks
- Aggregated residual transformations for deep neural networks (ResNeXt)
- Deep networks with stochastic depth
- FractalNet
- DenseNet
- SqueezeNet




