---
layout: pagecollection
title: 10 - Recurrent Neural Networks
collection: StanfordCS231n
---
{% include JB/setup %}

# Overview
Fundamental idea is that we move away from one-to-one networks i.e. we receive one image, and output one label, to many-to-many networks i.e. we receive one image, but we output a caption of variable length.

Other examples include taking in a variable-length sentence and outputting a single sentiment (positive or negative); in computer vision we may have a video which we treat as a sequence of frames, and we may want to classify which single activity is taking place in the video; and in machine translation where we may have a sequence of words in one language, and want to output some other sequence of another language.

# Architecture

The core idea is that there are RNN cells, which take an input at some time $$t$$ and update their internal state. We then use this to produce a vector at some time steps.

Fundamentally, we are applying a recurrence relation of the form:

$$h_t = f_W(h_{t-1}, x_t)$$

where: $$h_t$$ is the new state; $$h_{t-1}$$ is the old state; $$f_W$$ is some function with parameters $$W$$; and $$x_t$$ is some input vector at time $$t$$. Note that $$W$$ is not associated with a time step, it is the same weight matrix through time that we update.

### Unrolling

The recurrence relation is often hard to visualise, so the network is often drawn in its _unrolled_ form, through time.

### Sequence to Sequence

This can be thought of as many-to-one + one-to-many. That is, we summarise the input, by encoding it in a single vector after the first many-to-one model, then decode it in the many-to-one model, and produce a variable length output.

### Backpropagation through time

Instead of waiting for the entire input sequence and then backpropagating through this, we truncate the input, compute the backprop over this truncated input, update our weights, and then continue in this manner until we have completed then whole sequence.

A short example of this can be found [here](https://gist.github.com/karpathy/d4dee566867f8291f086).

### Vanilla gradient flow
Since we we are backpropagating through time, if we follow the gradients we will see that there is repeated multiplication by $$W$$. This will lead to either exploding or vanishing gradients.

# Captioning

In Computer Vision, it can be useful to generate captions. One way of doing this is to train a CNN on an image, but instead of producing a softmax output, we pass the final fully connected layer's weight matrix into an RNN state $$h_0$$.

This works by seeding an RNN with some initialisation, and letting it generate words until some special END token is reached.

# Long Short-Tem Memory (LSTM)

LSTMs maintain two vectors of memory, the $$h_t$$ we have seen, but also a $$c_t$$ vector as well.

It has a slightly complicated structure:

$$
\left( \begin{array}{c}
i \\
f \\
o \\
g \\
\end{array} \right)
=
\left( \begin{array}{c}
\sigma \\
\sigma \\
\sigma \\
tanh \\
\end{array} \right)
W
\left( \begin{array}{c}
h_{t-1} \\
x_t \\
\end{array} \right)
$$

Each of these terms, $$i, f, g, o$$ has a different purpose:

- $$f$$, **forget gate** whether to erase the cell
- $$i$$, **input gate** whether to write to the cell
- $$g$$, **gate gate** how much to write to cell
- $$o$$, **output gate** how much to reveal of the cell

### Other RNN variants

Gated Recurrent Units (GRUs) are an alternative to LSTMs which also attempt to avoid the vanishing gradient problem.
