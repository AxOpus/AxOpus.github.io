---
layout: pagecollection
title: Week 10 - Large Datasets
collection: StanfordML
---
{% include JB/setup %}
## Why so large?
We've seen that in many cases, having large amounts of data can produce better results, even for a sub-optimal algorithm. We can check if this is likely to be the case by using a small sample of the data and observe our algorithms performance.

## Gradient descent
We have so far used GD on the whole dataset. For situations where we have a large amount of data (>100MM data points), we would require this to be loaded into memory, which may not be possible.

#### Stochastic gradient descent
We can alter GD by using stochastic gradient descent, where we update after every example. The steps are:

```Python
Randomly shuffle dataset
Repeat{
    for i = 1 ... m{
        theta_j = theta_j - alpha * (h_x_i - y_i) * x_j_i
    }
}
```

#### Mini-batch gradient descent
We can also take an in-between approach, and define batches of size $$b$$, usually $$2 \leq b \leq 100$$.

## Online learning
SUppose we have a continuous stream of data, then we might want to update the algorithm to reflect the users' change over time. We can use SGD to update after each customer interaction.

## MapReduce and data parallelism
We can use the MapReduce paradigm to significantly speed-up work, by parallelising the algorithm. Instead of having one machine sum over the $$m$$ training examples, we could instead have each machine sum over $$\frac{m}{num\_machines}$$ examples, then sum the sums and perform a gradient descent update.




















